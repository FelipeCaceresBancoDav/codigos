{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow 2.0 OD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUl8qfjttvBj",
        "outputId": "03d06566-f976-445d-e197-a6c5011d328b"
      },
      "source": [
        "!update-alternatives --install /usr/bin/python python /usr/bin/python3.8.6 1\n",
        "!update-alternatives --list python\n",
        "!sudo update-alternatives --config python\n",
        "!sudo update-alternatives --set python /usr/bin/python3.8\n",
        "!python3 --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: error: alternative path /usr/bin/python3.8.6 doesn't exist\n",
            "update-alternatives: error: no alternatives for python\n",
            "update-alternatives: error: no alternatives for python\n",
            "update-alternatives: error: no alternatives for python\n",
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvB91ItKS5xQ",
        "outputId": "c528bd36-0ab1-4a32-f1ab-1d00a4435530"
      },
      "source": [
        "!pip install tensorflow-gpu==2.4.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-gpu==2.4.0\n",
            "  Downloading tensorflow_gpu-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (0.12.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.15.0)\n",
            "Collecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (0.2.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.12.1)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 34.4 MB/s \n",
            "\u001b[?25hCollecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 39.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (2.6.0)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (0.37.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu==2.4.0) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu==2.4.0) (3.6.0)\n",
            "Installing collected packages: grpcio, tensorflow-estimator, h5py, gast, tensorflow-gpu\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.41.0\n",
            "    Uninstalling grpcio-1.41.0:\n",
            "      Successfully uninstalled grpcio-1.41.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires gast==0.4.0, but you have gast 0.3.3 which is incompatible.\n",
            "tensorflow 2.6.0 requires grpcio<2.0,>=1.37.0, but you have grpcio 1.32.0 which is incompatible.\n",
            "tensorflow 2.6.0 requires h5py~=3.1.0, but you have h5py 2.10.0 which is incompatible.\n",
            "tensorflow 2.6.0 requires tensorflow-estimator~=2.6, but you have tensorflow-estimator 2.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 tensorflow-estimator-2.4.0 tensorflow-gpu-2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ovtwfLaTFPP",
        "outputId": "48fbe0fa-c967-4b49-a4fa-498a2978541e"
      },
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 66053, done.\u001b[K\n",
            "remote: Counting objects: 100% (748/748), done.\u001b[K\n",
            "remote: Compressing objects: 100% (292/292), done.\u001b[K\n",
            "remote: Total 66053 (delta 553), reused 643 (delta 456), pack-reused 65305\u001b[K\n",
            "Receiving objects: 100% (66053/66053), 575.75 MiB | 16.79 MiB/s, done.\n",
            "Resolving deltas: 100% (46263/46263), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxDLNd5hVjnV",
        "outputId": "20d1ffb4-f1af-486f-ed86-563b816487c7"
      },
      "source": [
        "%cd /content/models/research"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ96R77_VlqE"
      },
      "source": [
        "# From within TensorFlow/models/research/\n",
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVlKpgFiV0GN",
        "outputId": "a303b9c4-8874-441c-aeb2-ec42e102dd60"
      },
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cocoapi'...\n",
            "remote: Enumerating objects: 975, done.\u001b[K\n",
            "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\n",
            "Receiving objects: 100% (975/975), 11.72 MiB | 12.08 MiB/s, done.\n",
            "Resolving deltas: 100% (576/576), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GyhtrhlnWOSS",
        "outputId": "71913249-86e2-401b-c1e7-fc2893c2912c"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/models/research'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzq9mTwmWGbb",
        "outputId": "5d7cef8b-a4e0-45e2-920d-2224b776693c"
      },
      "source": [
        "%cd /content/models/research/cocoapi/PythonAPI\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research/cocoapi/PythonAPI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3hcmMqFPWTq7",
        "outputId": "16e1510f-e753-4882-9dd5-4cadfa34e610"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/models/research/cocoapi/PythonAPI'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXIx2RtKWRAU",
        "outputId": "a168481d-8305-420d-85ff-0cfcf94147db"
      },
      "source": [
        "!make"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python setup.py build_ext --inplace\n",
            "running build_ext\n",
            "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
            "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/models/research/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "building 'pycocotools._mask' extension\n",
            "creating build\n",
            "creating build/common\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.7/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
            "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
            "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
            "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
            "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
            "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
            "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
            "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
            "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
            "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
            "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/../common/maskApi.o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -o build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so -> pycocotools\n",
            "rm -rf build\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSowz_RnV-Az"
      },
      "source": [
        "!cp -r pycocotools /content/models/research"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtZ68ly9WmTd",
        "outputId": "c672568e-a56e-4e63-d959-3bd25a3e1aa0"
      },
      "source": [
        "# From within TensorFlow/models/research/\n",
        "%cd /content/models/research\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install --use-feature=2020-resolver ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n",
            "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n",
            "Processing /content/models/research\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.33.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 11.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.24)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.2)\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.5)\n",
            "Collecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 36.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 31.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n",
            "Collecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.8)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Collecting opencv-python-headless\n",
            "  Downloading opencv_python_headless-4.5.4.58-cp37-cp37m-manylinux2014_x86_64.whl (47.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 34 kB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 44.0 MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 44.6 MB/s \n",
            "\u001b[?25hCollecting tensorflow-text>=2.5.0\n",
            "  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.19.5)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.26.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.17.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (57.4.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2018.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (5.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.5.30)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.10)\n",
            "Collecting gast==0.4.0\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
            "Collecting tensorflow-estimator~=2.6\n",
            "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
            "Collecting h5py~=3.1.0\n",
            "  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 41.4 MB/s \n",
            "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
            "  Downloading grpcio-1.41.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 41.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.7.4.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (5.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.12.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (4.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.6)\n",
            "Collecting requests<3.0.0dev,>=2.18.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 835 kB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.12.0)\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "  Downloading fastavro-1.4.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 52.6 MB/s \n",
            "\u001b[?25hCollecting orjson<4.0\n",
            "  Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB)\n",
            "\u001b[K     |████████████████████████████████| 249 kB 37.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow<5.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.0.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 32.4 MB/s \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 33.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.6.0)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.3.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.10.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2019.12.20)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.2.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.2.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.2.0)\n",
            "Building wheels for collected packages: object-detection, py-cpuinfo, avro-python3, dill, future, seqeval\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1675727 sha256=d8ea8a0f3c1da806ef86805677e71986f803034bdda5f2bcb6770472e0d28a96\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a0v2ebji/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=4bc11dd1bbf8cf65c19b72015ee9c825ab2bf96f24a3c6b2db72f97554a3737c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=066dbf903dd94ee30df554884ac401669e0bd3ec0e188ae4f12395dc06aeacb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=8bdd65f2e6ddecc647c78d3125a3c6f7c8819e1d1ca7e2b0bf98bd9c835baa94\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=e52632c8b7671ab8cd613dfd15fea002d9d83aad2a67e4a1b362339d21a0f65a\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=851b5305e143cc4c12617b00df17da01171dfed7c37cfe11d0ce44b870b7d8d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built object-detection py-cpuinfo avro-python3 dill future seqeval\n",
            "Installing collected packages: requests, grpcio, tensorflow-estimator, h5py, gast, portalocker, future, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, py-cpuinfo, orjson, opencv-python-headless, hdfs, fastavro, avro-python3, tf-models-official, lvis, apache-beam, object-detection\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.32.0\n",
            "    Uninstalling grpcio-1.32.0:\n",
            "      Successfully uninstalled grpcio-1.32.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-gpu 2.4.0 requires gast==0.3.3, but you have gast 0.4.0 which is incompatible.\n",
            "tensorflow-gpu 2.4.0 requires grpcio~=1.32.0, but you have grpcio 1.41.1 which is incompatible.\n",
            "tensorflow-gpu 2.4.0 requires h5py~=2.10.0, but you have h5py 3.1.0 which is incompatible.\n",
            "tensorflow-gpu 2.4.0 requires tensorflow-estimator<2.5.0,>=2.4.0rc0, but you have tensorflow-estimator 2.6.0 which is incompatible.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed apache-beam-2.33.0 avro-python3-1.9.2.1 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.6 future-0.18.2 gast-0.4.0 grpcio-1.41.1 h5py-3.1.0 hdfs-2.6.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.5.4.58 orjson-3.6.4 portalocker-2.3.2 py-cpuinfo-8.0.0 pyyaml-6.0 requests-2.26.0 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorflow-addons-0.14.0 tensorflow-estimator-2.6.0 tensorflow-model-optimization-0.7.0 tensorflow-text-2.6.0 tf-models-official-2.6.0 tf-slim-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhnKCTvniH5R",
        "outputId": "777ce2fb-9bec-4e53-eb3f-9662ca8cae40"
      },
      "source": [
        "!cd /content/\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34madversarial_text\u001b[0m/    \u001b[01;34mcvt_text\u001b[0m/       \u001b[01;34mlstm_object_detection\u001b[0m/  README.md\n",
            "\u001b[01;34mattention_ocr\u001b[0m/       \u001b[01;34mdeeplab\u001b[0m/        \u001b[01;34mmarco\u001b[0m/                  \u001b[01;34mrebar\u001b[0m/\n",
            "\u001b[01;34maudioset\u001b[0m/            \u001b[01;34mdeep_speech\u001b[0m/    \u001b[01;34mnst_blogpost\u001b[0m/           \u001b[01;34mseq_flow_lite\u001b[0m/\n",
            "\u001b[01;34mautoaugment\u001b[0m/         \u001b[01;34mdelf\u001b[0m/           \u001b[01;34mobject_detection\u001b[0m/       setup.py\n",
            "\u001b[01;34mcocoapi\u001b[0m/             \u001b[01;34mefficient-hrl\u001b[0m/  \u001b[01;34mpcl_rl\u001b[0m/                 \u001b[01;34mslim\u001b[0m/\n",
            "\u001b[01;34mcognitive_planning\u001b[0m/  \u001b[01;34mlfads\u001b[0m/          \u001b[01;34mpycocotools\u001b[0m/            \u001b[01;34mvid2depth\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ukj0XaYttay"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG8e6r6DtP1H",
        "outputId": "f032f3a6-824f-4067-8ad4-8b69496311ea"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gS4tpt1W19b",
        "outputId": "a4b1637d-7866-445e-fe58-3a77745813ca"
      },
      "source": [
        "# From within TensorFlow/models/research/\n",
        "%cd /content/models/research\n",
        "!python object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/models/research\n",
            "2021-10-29 20:16:24.231203: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Traceback (most recent call last):\n",
            "  File \"object_detection/builders/model_builder_tf2_test.py\", line 22, in <module>\n",
            "    import tensorflow.compat.v1 as tf\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 41, in <module>\n",
            "    from tensorflow.python.tools import module_util as _module_util\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\", line 48, in <module>\n",
            "    from tensorflow.python import keras\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/__init__.py\", line 27, in <module>\n",
            "    from tensorflow.python.keras import models\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/models.py\", line 27, in <module>\n",
            "    from tensorflow.python.keras.engine import sequential\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py\", line 28, in <module>\n",
            "    from tensorflow.python.keras import layers as layer_module\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/__init__.py\", line 177, in <module>\n",
            "    from tensorflow.python.keras.layers.normalization import LayerNormalization\n",
            "ImportError: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBXGjZZ9XKbu"
      },
      "source": [
        "Listo, ahora para el modelo custom:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqtw5IILX4bA"
      },
      "source": [
        "import os\n",
        "if os.path.exists('/content/training_demo') == False:\n",
        "  os.mkdir('/content/training_demo') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpELv4U7XMOt"
      },
      "source": [
        "os.mkdir(\"/content/training_demo/annotations\")\n",
        "os.mkdir(\"/content/training_demo/exported_models\")\n",
        "os.mkdir(\"/content/training_demo/images\")\n",
        "os.mkdir(\"/content/training_demo/models\")\n",
        "os.mkdir(\"/content/training_demo/pre-trained-models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZoaErR3YSkY"
      },
      "source": [
        "os.mkdir(\"/content/training_demo/images/train\")\n",
        "os.mkdir(\"/content/training_demo/images/test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxewwcSFZLcH",
        "outputId": "7d081bd8-e1fa-4e9e-901b-5448736583d0"
      },
      "source": [
        "%cd /content/training_demo/pre-trained-models\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d5_coco17_tpu-32.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/training_demo/pre-trained-models\n",
            "--2021-10-29 06:35:59--  http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d5_coco17_tpu-32.tar.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 64.233.189.128, 2404:6800:4008:c06::80\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|64.233.189.128|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 256286417 (244M) [application/x-tar]\n",
            "Saving to: ‘efficientdet_d5_coco17_tpu-32.tar.gz’\n",
            "\n",
            "efficientdet_d5_coc 100%[===================>] 244.41M  65.6MB/s    in 3.7s    \n",
            "\n",
            "2021-10-29 06:36:04 (65.6 MB/s) - ‘efficientdet_d5_coco17_tpu-32.tar.gz’ saved [256286417/256286417]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5fPEaZ_ZbYv",
        "outputId": "c4119bc7-59bd-41af-e6f6-056565d8dce6"
      },
      "source": [
        "!tar -xvf /content/training_demo/pre-trained-models/efficientdet_d5_coco17_tpu-32.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "efficientdet_d5_coco17_tpu-32/\n",
            "efficientdet_d5_coco17_tpu-32/checkpoint/\n",
            "efficientdet_d5_coco17_tpu-32/checkpoint/ckpt-0.data-00000-of-00001\n",
            "efficientdet_d5_coco17_tpu-32/checkpoint/checkpoint\n",
            "efficientdet_d5_coco17_tpu-32/checkpoint/ckpt-0.index\n",
            "efficientdet_d5_coco17_tpu-32/pipeline.config\n",
            "efficientdet_d5_coco17_tpu-32/saved_model/\n",
            "efficientdet_d5_coco17_tpu-32/saved_model/saved_model.pb\n",
            "efficientdet_d5_coco17_tpu-32/saved_model/assets/\n",
            "efficientdet_d5_coco17_tpu-32/saved_model/variables/\n",
            "efficientdet_d5_coco17_tpu-32/saved_model/variables/variables.data-00000-of-00001\n",
            "efficientdet_d5_coco17_tpu-32/saved_model/variables/variables.index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dokNU3y2Z1E8",
        "outputId": "dd326dd8-f0d1-47cd-d87a-f3a4d8ab377f"
      },
      "source": [
        "%cd /content/training_demo/annotations\n",
        "!echo \"item { id: 1 name: 'titulo_seccion'}\"> label_map.pbtxt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/training_demo/annotations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HpMF7mspXyp"
      },
      "source": [
        "Subir zip con archivos, descomprimir y pasar a train y test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRJ_KGrRpcqA",
        "outputId": "6f070ad2-0dbe-4575-aa73-7a4c27a74710"
      },
      "source": [
        "%cd /content/\n",
        "!unzip todo.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Archive:  todo.zip\n",
            "  inflating: train/azynmtcc_pag_0.jpg  \n",
            "  inflating: train/azynmtcc_pag_0.xml  \n",
            "  inflating: train/azynmtcc_pag_1.jpg  \n",
            "  inflating: train/azynmtcc_pag_1.xml  \n",
            "  inflating: train/azynmtcc_pag_2.jpg  \n",
            "  inflating: train/azynmtcc_pag_2.xml  \n",
            "  inflating: train/azynmtcc_pag_3.jpg  \n",
            "  inflating: train/azynmtcc_pag_3.xml  \n",
            "  inflating: train/bamubwra_pag_0.jpg  \n",
            "  inflating: train/bamubwra_pag_0.xml  \n",
            "  inflating: train/bamubwra_pag_1.jpg  \n",
            "  inflating: train/bamubwra_pag_1.xml  \n",
            "  inflating: train/bamubwra_pag_10.jpg  \n",
            "  inflating: train/bamubwra_pag_10.xml  \n",
            "  inflating: train/bamubwra_pag_12.jpg  \n",
            "  inflating: train/bamubwra_pag_12.xml  \n",
            "  inflating: train/bamubwra_pag_13.jpg  \n",
            "  inflating: train/bamubwra_pag_13.xml  \n",
            "  inflating: train/bamubwra_pag_9.jpg  \n",
            "  inflating: train/bamubwra_pag_9.xml  \n",
            "  inflating: train/bcvrygbv_pag_0.jpg  \n",
            "  inflating: train/bcvrygbv_pag_0.xml  \n",
            "  inflating: train/bcvrygbv_pag_1.jpg  \n",
            "  inflating: train/bcvrygbv_pag_1.xml  \n",
            "  inflating: train/bcvrygbv_pag_2.jpg  \n",
            "  inflating: train/bcvrygbv_pag_2.xml  \n",
            "  inflating: train/bgixinkk_pag_0.jpg  \n",
            "  inflating: train/bgixinkk_pag_0.xml  \n",
            "  inflating: train/bgixinkk_pag_1.jpg  \n",
            "  inflating: train/bgixinkk_pag_1.xml  \n",
            "  inflating: train/bpahfbdk_pag_0.jpg  \n",
            "  inflating: train/bpahfbdk_pag_0.xml  \n",
            "  inflating: train/bpahfbdk_pag_1.jpg  \n",
            "  inflating: train/bpahfbdk_pag_1.xml  \n",
            "  inflating: train/bpahfbdk_pag_2.jpg  \n",
            "  inflating: train/bpahfbdk_pag_2.xml  \n",
            "  inflating: train/bpahfbdk_pag_3.jpg  \n",
            "  inflating: train/bpahfbdk_pag_3.xml  \n",
            "  inflating: train/brhvcjkz_pag_0.jpg  \n",
            "  inflating: train/brhvcjkz_pag_0.xml  \n",
            "  inflating: train/brhvcjkz_pag_1.jpg  \n",
            "  inflating: train/brhvcjkz_pag_1.xml  \n",
            "  inflating: train/brhvcjkz_pag_7.jpg  \n",
            "  inflating: train/brhvcjkz_pag_7.xml  \n",
            "  inflating: train/brhvcjkz_pag_8.jpg  \n",
            "  inflating: train/brhvcjkz_pag_8.xml  \n",
            "  inflating: train/brhvcjkz_pag_9.jpg  \n",
            "  inflating: train/brhvcjkz_pag_9.xml  \n",
            "  inflating: train/bsdrzlfc_pag_0.jpg  \n",
            "  inflating: train/bsdrzlfc_pag_0.xml  \n",
            "  inflating: train/bsdrzlfc_pag_1.jpg  \n",
            "  inflating: train/bsdrzlfc_pag_1.xml  \n",
            "  inflating: train/bsdrzlfc_pag_2.jpg  \n",
            "  inflating: train/bsdrzlfc_pag_2.xml  \n",
            "  inflating: train/bsdrzlfc_pag_3.jpg  \n",
            "  inflating: train/bsdrzlfc_pag_3.xml  \n",
            "  inflating: train/bsdrzlfc_pag_7.jpg  \n",
            "  inflating: train/bsdrzlfc_pag_7.xml  \n",
            "  inflating: train/bufkqoon_pag_1.jpg  \n",
            "  inflating: train/bufkqoon_pag_1.xml  \n",
            "  inflating: train/bufkqoon_pag_4.jpg  \n",
            "  inflating: train/bufkqoon_pag_4.xml  \n",
            "  inflating: train/bufkqoon_pag_5.jpg  \n",
            "  inflating: train/bufkqoon_pag_5.xml  \n",
            "  inflating: train/cedhcalv_pag_0.jpg  \n",
            "  inflating: train/cedhcalv_pag_0.xml  \n",
            "  inflating: train/cedhcalv_pag_1.jpg  \n",
            "  inflating: train/cedhcalv_pag_1.xml  \n",
            "  inflating: train/cedhcalv_pag_2.jpg  \n",
            "  inflating: train/cedhcalv_pag_2.xml  \n",
            "  inflating: train/cedhcalv_pag_3.jpg  \n",
            "  inflating: train/cedhcalv_pag_3.xml  \n",
            "  inflating: train/ckhcvwty_pag_0.jpg  \n",
            "  inflating: train/ckhcvwty_pag_0.xml  \n",
            "  inflating: train/ckhcvwty_pag_2.jpg  \n",
            "  inflating: train/ckhcvwty_pag_2.xml  \n",
            "  inflating: train/cktusati_pag_1.jpg  \n",
            "  inflating: train/cktusati_pag_1.xml  \n",
            "  inflating: train/cktusati_pag_2.jpg  \n",
            "  inflating: train/cktusati_pag_2.xml  \n",
            "  inflating: train/cktusati_pag_3.jpg  \n",
            "  inflating: train/cktusati_pag_3.xml  \n",
            "  inflating: train/cktusati_pag_4.jpg  \n",
            "  inflating: train/cktusati_pag_4.xml  \n",
            "  inflating: train/cptrudai_pag_0.jpg  \n",
            "  inflating: train/cptrudai_pag_0.xml  \n",
            "  inflating: train/cptrudai_pag_1.jpg  \n",
            "  inflating: train/cptrudai_pag_1.xml  \n",
            "  inflating: train/cptrudai_pag_3.jpg  \n",
            "  inflating: train/cptrudai_pag_3.xml  \n",
            "  inflating: train/crqwnejj_pag_0.jpg  \n",
            "  inflating: train/crqwnejj_pag_0.xml  \n",
            "  inflating: train/crqwnejj_pag_1.jpg  \n",
            "  inflating: train/crqwnejj_pag_1.xml  \n",
            "  inflating: train/crqwnejj_pag_2.jpg  \n",
            "  inflating: train/crqwnejj_pag_2.xml  \n",
            "  inflating: train/crqwnejj_pag_3.jpg  \n",
            "  inflating: train/crqwnejj_pag_3.xml  \n",
            "  inflating: train/crqwnejj_pag_4.jpg  \n",
            "  inflating: train/crqwnejj_pag_4.xml  \n",
            "  inflating: train/cufukovg_pag_0.jpg  \n",
            "  inflating: train/cufukovg_pag_0.xml  \n",
            "  inflating: train/cufukovg_pag_1.jpg  \n",
            "  inflating: train/cufukovg_pag_1.xml  \n",
            "  inflating: train/cufukovg_pag_2.jpg  \n",
            "  inflating: train/cufukovg_pag_2.xml  \n",
            "  inflating: train/cufukovg_pag_3.jpg  \n",
            "  inflating: train/cufukovg_pag_3.xml  \n",
            "  inflating: train/cufukovg_pag_4.jpg  \n",
            "  inflating: train/cufukovg_pag_4.xml  \n",
            "  inflating: train/cufukovg_pag_6.jpg  \n",
            "  inflating: train/cufukovg_pag_6.xml  \n",
            "  inflating: train/cufukovg_pag_7.jpg  \n",
            "  inflating: train/cufukovg_pag_7.xml  \n",
            "  inflating: train/cufukovg_pag_8.jpg  \n",
            "  inflating: train/cufukovg_pag_8.xml  \n",
            "  inflating: train/cwrfsttn_pag_0.jpg  \n",
            "  inflating: train/cwrfsttn_pag_0.xml  \n",
            "  inflating: train/cwrfsttn_pag_1.jpg  \n",
            "  inflating: train/cwrfsttn_pag_1.xml  \n",
            "  inflating: train/cwrfsttn_pag_2.jpg  \n",
            "  inflating: train/cwrfsttn_pag_2.xml  \n",
            "  inflating: train/cwrfsttn_pag_3.jpg  \n",
            "  inflating: train/cwrfsttn_pag_3.xml  \n",
            "  inflating: train/dqltdefw_pag_0.jpg  \n",
            "  inflating: train/dqltdefw_pag_0.xml  \n",
            "  inflating: train/dqltdefw_pag_1.jpg  \n",
            "  inflating: train/dqltdefw_pag_1.xml  \n",
            "  inflating: train/dqltdefw_pag_2.jpg  \n",
            "  inflating: train/dqltdefw_pag_2.xml  \n",
            "  inflating: train/drzejkwd_pag_0.jpg  \n",
            "  inflating: train/drzejkwd_pag_0.xml  \n",
            "  inflating: train/drzejkwd_pag_3.jpg  \n",
            "  inflating: train/drzejkwd_pag_3.xml  \n",
            "  inflating: train/drzejkwd_pag_4.jpg  \n",
            "  inflating: train/drzejkwd_pag_4.xml  \n",
            "  inflating: train/drzejkwd_pag_6.jpg  \n",
            "  inflating: train/drzejkwd_pag_6.xml  \n",
            "  inflating: train/drzejkwd_pag_7.jpg  \n",
            "  inflating: train/drzejkwd_pag_7.xml  \n",
            "  inflating: train/dxkzmfbi_pag_0.jpg  \n",
            "  inflating: train/dxkzmfbi_pag_0.xml  \n",
            "  inflating: train/dxkzmfbi_pag_1.jpg  \n",
            "  inflating: train/dxkzmfbi_pag_1.xml  \n",
            "  inflating: train/dxkzmfbi_pag_2.jpg  \n",
            "  inflating: train/dxkzmfbi_pag_2.xml  \n",
            "  inflating: train/dxkzmfbi_pag_3.jpg  \n",
            "  inflating: train/dxkzmfbi_pag_3.xml  \n",
            "  inflating: train/dxkzmfbi_pag_4.jpg  \n",
            "  inflating: train/dxkzmfbi_pag_4.xml  \n",
            "  inflating: train/dyguiepb_pag_0.jpg  \n",
            "  inflating: train/dyguiepb_pag_0.xml  \n",
            "  inflating: train/dyguiepb_pag_1.jpg  \n",
            "  inflating: train/dyguiepb_pag_1.xml  \n",
            "  inflating: train/dyguiepb_pag_2.jpg  \n",
            "  inflating: train/dyguiepb_pag_2.xml  \n",
            "  inflating: train/dyguiepb_pag_3.jpg  \n",
            "  inflating: train/dyguiepb_pag_3.xml  \n",
            "  inflating: train/dyguiepb_pag_4.jpg  \n",
            "  inflating: train/dyguiepb_pag_4.xml  \n",
            "  inflating: train/egombrag_pag_0.jpg  \n",
            "  inflating: train/egombrag_pag_0.xml  \n",
            "  inflating: train/egombrag_pag_1.jpg  \n",
            "  inflating: train/egombrag_pag_1.xml  \n",
            "  inflating: train/egombrag_pag_2.jpg  \n",
            "  inflating: train/egombrag_pag_2.xml  \n",
            "  inflating: train/egombrag_pag_3.jpg  \n",
            "  inflating: train/egombrag_pag_3.xml  \n",
            "  inflating: train/egombrag_pag_4.jpg  \n",
            "  inflating: train/egombrag_pag_4.xml  \n",
            "  inflating: train/egombrag_pag_5.jpg  \n",
            "  inflating: train/egombrag_pag_5.xml  \n",
            "  inflating: train/egombrag_pag_6.jpg  \n",
            "  inflating: train/egombrag_pag_6.xml  \n",
            "  inflating: train/eiwdqedd_pag_0.jpg  \n",
            "  inflating: train/eiwdqedd_pag_0.xml  \n",
            "  inflating: train/eiwdqedd_pag_1.jpg  \n",
            "  inflating: train/eiwdqedd_pag_1.xml  \n",
            "  inflating: train/esunisko_pag_0.jpg  \n",
            "  inflating: train/esunisko_pag_0.xml  \n",
            "  inflating: train/esunisko_pag_1.jpg  \n",
            "  inflating: train/esunisko_pag_1.xml  \n",
            "  inflating: train/esunisko_pag_2.jpg  \n",
            "  inflating: train/esunisko_pag_2.xml  \n",
            "  inflating: train/esunisko_pag_3.jpg  \n",
            "  inflating: train/esunisko_pag_3.xml  \n",
            "  inflating: train/esunisko_pag_4.jpg  \n",
            "  inflating: train/esunisko_pag_4.xml  \n",
            "  inflating: train/esunisko_pag_6.jpg  \n",
            "  inflating: train/esunisko_pag_6.xml  \n",
            "  inflating: train/evozzvuy_pag_0.jpg  \n",
            "  inflating: train/evozzvuy_pag_0.xml  \n",
            "  inflating: train/evozzvuy_pag_1.jpg  \n",
            "  inflating: train/evozzvuy_pag_1.xml  \n",
            "  inflating: train/evozzvuy_pag_3.jpg  \n",
            "  inflating: train/evozzvuy_pag_3.xml  \n",
            "  inflating: train/evozzvuy_pag_5.jpg  \n",
            "  inflating: train/evozzvuy_pag_5.xml  \n",
            "  inflating: train/evozzvuy_pag_6.jpg  \n",
            "  inflating: train/evozzvuy_pag_6.xml  \n",
            "  inflating: train/evozzvuy_pag_7.jpg  \n",
            "  inflating: train/evozzvuy_pag_7.xml  \n",
            "  inflating: train/fancyels_pag_0.jpg  \n",
            "  inflating: train/fancyels_pag_0.xml  \n",
            "  inflating: train/fancyels_pag_1.jpg  \n",
            "  inflating: train/fancyels_pag_1.xml  \n",
            "  inflating: train/fancyels_pag_2.jpg  \n",
            "  inflating: train/fancyels_pag_2.xml  \n",
            "  inflating: train/fancyels_pag_3.jpg  \n",
            "  inflating: train/fancyels_pag_3.xml  \n",
            "  inflating: train/fancyels_pag_4.jpg  \n",
            "  inflating: train/fancyels_pag_4.xml  \n",
            "  inflating: train/fancyels_pag_5.jpg  \n",
            "  inflating: train/fancyels_pag_5.xml  \n",
            "  inflating: train/fdbxdpnw_pag_0.jpg  \n",
            "  inflating: train/fdbxdpnw_pag_0.xml  \n",
            "  inflating: train/fdbxdpnw_pag_1.jpg  \n",
            "  inflating: train/fdbxdpnw_pag_1.xml  \n",
            "  inflating: train/fdbxdpnw_pag_2.jpg  \n",
            "  inflating: train/fdbxdpnw_pag_2.xml  \n",
            "  inflating: train/fdbxdpnw_pag_3.jpg  \n",
            "  inflating: train/fdbxdpnw_pag_3.xml  \n",
            "  inflating: train/fdbxdpnw_pag_4.jpg  \n",
            "  inflating: train/fdbxdpnw_pag_4.xml  \n",
            "  inflating: train/fqyvjlms_pag_0.jpg  \n",
            "  inflating: train/fqyvjlms_pag_0.xml  \n",
            "  inflating: train/fqyvjlms_pag_1.jpg  \n",
            "  inflating: train/fqyvjlms_pag_1.xml  \n",
            "  inflating: train/fqyvjlms_pag_2.jpg  \n",
            "  inflating: train/fqyvjlms_pag_2.xml  \n",
            "  inflating: train/gctolffb_pag_0.jpg  \n",
            "  inflating: train/gctolffb_pag_0.xml  \n",
            "  inflating: train/gctolffb_pag_1.jpg  \n",
            "  inflating: train/gctolffb_pag_1.xml  \n",
            "  inflating: train/gctolffb_pag_11.jpg  \n",
            "  inflating: train/gctolffb_pag_11.xml  \n",
            "  inflating: train/gctolffb_pag_12.jpg  \n",
            "  inflating: train/gctolffb_pag_12.xml  \n",
            "  inflating: train/gctolffb_pag_13.jpg  \n",
            "  inflating: train/gctolffb_pag_13.xml  \n",
            "  inflating: train/gctolffb_pag_14.jpg  \n",
            "  inflating: train/gctolffb_pag_14.xml  \n",
            "  inflating: train/gctolffb_pag_2.jpg  \n",
            "  inflating: train/gctolffb_pag_2.xml  \n",
            "  inflating: train/gctolffb_pag_3.jpg  \n",
            "  inflating: train/gctolffb_pag_3.xml  \n",
            "  inflating: train/gctolffb_pag_4.jpg  \n",
            "  inflating: train/gctolffb_pag_4.xml  \n",
            "  inflating: train/gctolffb_pag_9.jpg  \n",
            "  inflating: train/gctolffb_pag_9.xml  \n",
            "  inflating: train/gfecfhty_pag_0.jpg  \n",
            "  inflating: train/gfecfhty_pag_0.xml  \n",
            "  inflating: train/gfecfhty_pag_1.jpg  \n",
            "  inflating: train/gfecfhty_pag_1.xml  \n",
            "  inflating: train/gfecfhty_pag_2.jpg  \n",
            "  inflating: train/gfecfhty_pag_2.xml  \n",
            "  inflating: train/gfecfhty_pag_3.jpg  \n",
            "  inflating: train/gfecfhty_pag_3.xml  \n",
            "  inflating: train/gfecfhty_pag_4.jpg  \n",
            "  inflating: train/gfecfhty_pag_4.xml  \n",
            "  inflating: train/gfecfhty_pag_5.jpg  \n",
            "  inflating: train/gfecfhty_pag_5.xml  \n",
            "  inflating: test/bamubwra_pag_11.jpg  \n",
            "  inflating: test/bamubwra_pag_11.xml  \n",
            "  inflating: test/bpahfbdk_pag_4.jpg  \n",
            "  inflating: test/bpahfbdk_pag_4.xml  \n",
            "  inflating: test/bsdrzlfc_pag_4.jpg  \n",
            "  inflating: test/bsdrzlfc_pag_4.xml  \n",
            "  inflating: test/bufkqoon_pag_0.jpg  \n",
            "  inflating: test/bufkqoon_pag_0.xml  \n",
            "  inflating: test/ckhcvwty_pag_1.jpg  \n",
            "  inflating: test/ckhcvwty_pag_1.xml  \n",
            "  inflating: test/cktusati_pag_0.jpg  \n",
            "  inflating: test/cktusati_pag_0.xml  \n",
            "  inflating: test/cufukovg_pag_5.jpg  \n",
            "  inflating: test/cufukovg_pag_5.xml  \n",
            "  inflating: test/cwrfsttn_pag_4.jpg  \n",
            "  inflating: test/cwrfsttn_pag_4.xml  \n",
            "  inflating: test/dqltdefw_pag_3.jpg  \n",
            "  inflating: test/dqltdefw_pag_3.xml  \n",
            "  inflating: test/dqltdefw_pag_4.jpg  \n",
            "  inflating: test/dqltdefw_pag_4.xml  \n",
            "  inflating: test/drzejkwd_pag_1.jpg  \n",
            "  inflating: test/drzejkwd_pag_1.xml  \n",
            "  inflating: test/drzejkwd_pag_5.jpg  \n",
            "  inflating: test/drzejkwd_pag_5.xml  \n",
            "  inflating: test/egombrag_pag_10.jpg  \n",
            "  inflating: test/egombrag_pag_10.xml  \n",
            "  inflating: test/esunisko_pag_5.jpg  \n",
            "  inflating: test/esunisko_pag_5.xml  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e14uV1Npb0x"
      },
      "source": [
        "!cp -a train/. /content/training_demo/images/train\n",
        "!cp -a test/. /content/training_demo/images/test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJCkaeFMnRCI",
        "outputId": "a9acd0e6-301f-4fb2-def3-184f6934b13b"
      },
      "source": [
        "%cd /content/training_demo/images/train\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/training_demo/images/train\n",
            "azynmtcc_pag_0.jpg   cptrudai_pag_3.jpg  eiwdqedd_pag_1.jpg\n",
            "azynmtcc_pag_0.xml   cptrudai_pag_3.xml  eiwdqedd_pag_1.xml\n",
            "azynmtcc_pag_1.jpg   crqwnejj_pag_0.jpg  esunisko_pag_0.jpg\n",
            "azynmtcc_pag_1.xml   crqwnejj_pag_0.xml  esunisko_pag_0.xml\n",
            "azynmtcc_pag_2.jpg   crqwnejj_pag_1.jpg  esunisko_pag_1.jpg\n",
            "azynmtcc_pag_2.xml   crqwnejj_pag_1.xml  esunisko_pag_1.xml\n",
            "azynmtcc_pag_3.jpg   crqwnejj_pag_2.jpg  esunisko_pag_2.jpg\n",
            "azynmtcc_pag_3.xml   crqwnejj_pag_2.xml  esunisko_pag_2.xml\n",
            "bamubwra_pag_0.jpg   crqwnejj_pag_3.jpg  esunisko_pag_3.jpg\n",
            "bamubwra_pag_0.xml   crqwnejj_pag_3.xml  esunisko_pag_3.xml\n",
            "bamubwra_pag_10.jpg  crqwnejj_pag_4.jpg  esunisko_pag_4.jpg\n",
            "bamubwra_pag_10.xml  crqwnejj_pag_4.xml  esunisko_pag_4.xml\n",
            "bamubwra_pag_12.jpg  cufukovg_pag_0.jpg  esunisko_pag_6.jpg\n",
            "bamubwra_pag_12.xml  cufukovg_pag_0.xml  esunisko_pag_6.xml\n",
            "bamubwra_pag_13.jpg  cufukovg_pag_1.jpg  evozzvuy_pag_0.jpg\n",
            "bamubwra_pag_13.xml  cufukovg_pag_1.xml  evozzvuy_pag_0.xml\n",
            "bamubwra_pag_1.jpg   cufukovg_pag_2.jpg  evozzvuy_pag_1.jpg\n",
            "bamubwra_pag_1.xml   cufukovg_pag_2.xml  evozzvuy_pag_1.xml\n",
            "bamubwra_pag_9.jpg   cufukovg_pag_3.jpg  evozzvuy_pag_3.jpg\n",
            "bamubwra_pag_9.xml   cufukovg_pag_3.xml  evozzvuy_pag_3.xml\n",
            "bcvrygbv_pag_0.jpg   cufukovg_pag_4.jpg  evozzvuy_pag_5.jpg\n",
            "bcvrygbv_pag_0.xml   cufukovg_pag_4.xml  evozzvuy_pag_5.xml\n",
            "bcvrygbv_pag_1.jpg   cufukovg_pag_6.jpg  evozzvuy_pag_6.jpg\n",
            "bcvrygbv_pag_1.xml   cufukovg_pag_6.xml  evozzvuy_pag_6.xml\n",
            "bcvrygbv_pag_2.jpg   cufukovg_pag_7.jpg  evozzvuy_pag_7.jpg\n",
            "bcvrygbv_pag_2.xml   cufukovg_pag_7.xml  evozzvuy_pag_7.xml\n",
            "bgixinkk_pag_0.jpg   cufukovg_pag_8.jpg  fancyels_pag_0.jpg\n",
            "bgixinkk_pag_0.xml   cufukovg_pag_8.xml  fancyels_pag_0.xml\n",
            "bgixinkk_pag_1.jpg   cwrfsttn_pag_0.jpg  fancyels_pag_1.jpg\n",
            "bgixinkk_pag_1.xml   cwrfsttn_pag_0.xml  fancyels_pag_1.xml\n",
            "bpahfbdk_pag_0.jpg   cwrfsttn_pag_1.jpg  fancyels_pag_2.jpg\n",
            "bpahfbdk_pag_0.xml   cwrfsttn_pag_1.xml  fancyels_pag_2.xml\n",
            "bpahfbdk_pag_1.jpg   cwrfsttn_pag_2.jpg  fancyels_pag_3.jpg\n",
            "bpahfbdk_pag_1.xml   cwrfsttn_pag_2.xml  fancyels_pag_3.xml\n",
            "bpahfbdk_pag_2.jpg   cwrfsttn_pag_3.jpg  fancyels_pag_4.jpg\n",
            "bpahfbdk_pag_2.xml   cwrfsttn_pag_3.xml  fancyels_pag_4.xml\n",
            "bpahfbdk_pag_3.jpg   dqltdefw_pag_0.jpg  fancyels_pag_5.jpg\n",
            "bpahfbdk_pag_3.xml   dqltdefw_pag_0.xml  fancyels_pag_5.xml\n",
            "brhvcjkz_pag_0.jpg   dqltdefw_pag_1.jpg  fdbxdpnw_pag_0.jpg\n",
            "brhvcjkz_pag_0.xml   dqltdefw_pag_1.xml  fdbxdpnw_pag_0.xml\n",
            "brhvcjkz_pag_1.jpg   dqltdefw_pag_2.jpg  fdbxdpnw_pag_1.jpg\n",
            "brhvcjkz_pag_1.xml   dqltdefw_pag_2.xml  fdbxdpnw_pag_1.xml\n",
            "brhvcjkz_pag_7.jpg   drzejkwd_pag_0.jpg  fdbxdpnw_pag_2.jpg\n",
            "brhvcjkz_pag_7.xml   drzejkwd_pag_0.xml  fdbxdpnw_pag_2.xml\n",
            "brhvcjkz_pag_8.jpg   drzejkwd_pag_3.jpg  fdbxdpnw_pag_3.jpg\n",
            "brhvcjkz_pag_8.xml   drzejkwd_pag_3.xml  fdbxdpnw_pag_3.xml\n",
            "brhvcjkz_pag_9.jpg   drzejkwd_pag_4.jpg  fdbxdpnw_pag_4.jpg\n",
            "brhvcjkz_pag_9.xml   drzejkwd_pag_4.xml  fdbxdpnw_pag_4.xml\n",
            "bsdrzlfc_pag_0.jpg   drzejkwd_pag_6.jpg  fqyvjlms_pag_0.jpg\n",
            "bsdrzlfc_pag_0.xml   drzejkwd_pag_6.xml  fqyvjlms_pag_0.xml\n",
            "bsdrzlfc_pag_1.jpg   drzejkwd_pag_7.jpg  fqyvjlms_pag_1.jpg\n",
            "bsdrzlfc_pag_1.xml   drzejkwd_pag_7.xml  fqyvjlms_pag_1.xml\n",
            "bsdrzlfc_pag_2.jpg   dxkzmfbi_pag_0.jpg  fqyvjlms_pag_2.jpg\n",
            "bsdrzlfc_pag_2.xml   dxkzmfbi_pag_0.xml  fqyvjlms_pag_2.xml\n",
            "bsdrzlfc_pag_3.jpg   dxkzmfbi_pag_1.jpg  gctolffb_pag_0.jpg\n",
            "bsdrzlfc_pag_3.xml   dxkzmfbi_pag_1.xml  gctolffb_pag_0.xml\n",
            "bsdrzlfc_pag_7.jpg   dxkzmfbi_pag_2.jpg  gctolffb_pag_11.jpg\n",
            "bsdrzlfc_pag_7.xml   dxkzmfbi_pag_2.xml  gctolffb_pag_11.xml\n",
            "bufkqoon_pag_1.jpg   dxkzmfbi_pag_3.jpg  gctolffb_pag_12.jpg\n",
            "bufkqoon_pag_1.xml   dxkzmfbi_pag_3.xml  gctolffb_pag_12.xml\n",
            "bufkqoon_pag_4.jpg   dxkzmfbi_pag_4.jpg  gctolffb_pag_13.jpg\n",
            "bufkqoon_pag_4.xml   dxkzmfbi_pag_4.xml  gctolffb_pag_13.xml\n",
            "bufkqoon_pag_5.jpg   dyguiepb_pag_0.jpg  gctolffb_pag_14.jpg\n",
            "bufkqoon_pag_5.xml   dyguiepb_pag_0.xml  gctolffb_pag_14.xml\n",
            "cedhcalv_pag_0.jpg   dyguiepb_pag_1.jpg  gctolffb_pag_1.jpg\n",
            "cedhcalv_pag_0.xml   dyguiepb_pag_1.xml  gctolffb_pag_1.xml\n",
            "cedhcalv_pag_1.jpg   dyguiepb_pag_2.jpg  gctolffb_pag_2.jpg\n",
            "cedhcalv_pag_1.xml   dyguiepb_pag_2.xml  gctolffb_pag_2.xml\n",
            "cedhcalv_pag_2.jpg   dyguiepb_pag_3.jpg  gctolffb_pag_3.jpg\n",
            "cedhcalv_pag_2.xml   dyguiepb_pag_3.xml  gctolffb_pag_3.xml\n",
            "cedhcalv_pag_3.jpg   dyguiepb_pag_4.jpg  gctolffb_pag_4.jpg\n",
            "cedhcalv_pag_3.xml   dyguiepb_pag_4.xml  gctolffb_pag_4.xml\n",
            "ckhcvwty_pag_0.jpg   egombrag_pag_0.jpg  gctolffb_pag_9.jpg\n",
            "ckhcvwty_pag_0.xml   egombrag_pag_0.xml  gctolffb_pag_9.xml\n",
            "ckhcvwty_pag_2.jpg   egombrag_pag_1.jpg  gfecfhty_pag_0.jpg\n",
            "ckhcvwty_pag_2.xml   egombrag_pag_1.xml  gfecfhty_pag_0.xml\n",
            "cktusati_pag_1.jpg   egombrag_pag_2.jpg  gfecfhty_pag_1.jpg\n",
            "cktusati_pag_1.xml   egombrag_pag_2.xml  gfecfhty_pag_1.xml\n",
            "cktusati_pag_2.jpg   egombrag_pag_3.jpg  gfecfhty_pag_2.jpg\n",
            "cktusati_pag_2.xml   egombrag_pag_3.xml  gfecfhty_pag_2.xml\n",
            "cktusati_pag_3.jpg   egombrag_pag_4.jpg  gfecfhty_pag_3.jpg\n",
            "cktusati_pag_3.xml   egombrag_pag_4.xml  gfecfhty_pag_3.xml\n",
            "cktusati_pag_4.jpg   egombrag_pag_5.jpg  gfecfhty_pag_4.jpg\n",
            "cktusati_pag_4.xml   egombrag_pag_5.xml  gfecfhty_pag_4.xml\n",
            "cptrudai_pag_0.jpg   egombrag_pag_6.jpg  gfecfhty_pag_5.jpg\n",
            "cptrudai_pag_0.xml   egombrag_pag_6.xml  gfecfhty_pag_5.xml\n",
            "cptrudai_pag_1.jpg   eiwdqedd_pag_0.jpg\n",
            "cptrudai_pag_1.xml   eiwdqedd_pag_0.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlsDuCvXn3Fx",
        "outputId": "50e9da46-5de0-43e5-d0a9-e0d27205c57e"
      },
      "source": [
        "%cd /content/training_demo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/training_demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqsuqlUEq-WL"
      },
      "source": [
        "#subir tf records en training demo, está en doc: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXCQfME6nxjI",
        "outputId": "2daf4764-1715-4c0e-f47f-5a194dd1fd15"
      },
      "source": [
        "# Create train data:\n",
        "!python generate_tfrecord.py -x images/train -l annotations/label_map.pbtxt -o annotations/train.record\n",
        "\n",
        "# Create test data:\n",
        "!python generate_tfrecord.py -x images/test -l annotations/label_map.pbtxt -o annotations/test.record\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"generate_tfrecord.py\", line 27, in <module>\n",
            "    import tensorflow.compat.v1 as tf\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 41, in <module>\n",
            "    from tensorflow.python.tools import module_util as _module_util\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\", line 47, in <module>\n",
            "    from tensorflow.python import keras\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/__init__.py\", line 27, in <module>\n",
            "    from tensorflow.python.keras import models\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/models.py\", line 27, in <module>\n",
            "    from tensorflow.python.keras.engine import sequential\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py\", line 27, in <module>\n",
            "    from tensorflow.python.keras import layers as layer_module\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/__init__.py\", line 174, in <module>\n",
            "    from tensorflow.python.keras.layers.normalization import LayerNormalization\n",
            "ImportError: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization/__init__.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"generate_tfrecord.py\", line 27, in <module>\n",
            "    import tensorflow.compat.v1 as tf\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 41, in <module>\n",
            "    from tensorflow.python.tools import module_util as _module_util\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\", line 47, in <module>\n",
            "    from tensorflow.python import keras\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/__init__.py\", line 27, in <module>\n",
            "    from tensorflow.python.keras import models\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/models.py\", line 27, in <module>\n",
            "    from tensorflow.python.keras.engine import sequential\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py\", line 27, in <module>\n",
            "    from tensorflow.python.keras import layers as layer_module\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/__init__.py\", line 174, in <module>\n",
            "    from tensorflow.python.keras.layers.normalization import LayerNormalization\n",
            "ImportError: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkerIrsdbzhK"
      },
      "source": [
        "os.mkdir(\"/content/training_demo/models/primer_modelo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr5kN5OHccbV"
      },
      "source": [
        "!cp /content/training_demo/pre-trained-models/efficientdet_d5_coco17_tpu-32/pipeline.config /content/training_demo/models/primer_modelo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d0VwxuZi8kV"
      },
      "source": [
        "#Hacer modificaciones a pipeline copiado: \n",
        "#clases, batch_size, fine_tune_checkpoint, fine_tune_checkpoint_type\n",
        "#label_map_path, input_path, use_bfloat16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-d1_0TZeZmm"
      },
      "source": [
        "#batch size recomendado para que funcione en colab, por lo menos con ssd: 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy1rZi_KhKIp"
      },
      "source": [
        "Entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACVyRfCUhLKh"
      },
      "source": [
        "!cp /content/models/research/object_detection/model_main_tf2.py /content/training_demo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmPK2ixyhSAZ",
        "outputId": "9f92d20c-6bc2-4fc4-d3fd-ced0267dc758"
      },
      "source": [
        "%cd /content/training_demo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/training_demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "S4fX9AKng7IR",
        "outputId": "4435ca6e-5af2-4ff4-c50f-83628d1b60da"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/training_demo'"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F88E_NJGtlrQ"
      },
      "source": [
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRtMlwUXukBO"
      },
      "source": [
        "import tensorflow\n",
        "gpu_devices = tensorflow.config.experimental.list_physical_devices('GPU') \n",
        "for device in gpu_devices:\n",
        "  tensorflow.config.experimental.set_memory_growth(device, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRyPwJEUg4WS",
        "outputId": "acce3e13-1116-41a2-ca5b-e8e328688414"
      },
      "source": [
        "!python model_main_tf2.py --model_dir=models/primer_modelo --pipeline_config_path=models/primer_modelo/pipeline.config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-10-29 06:28:34.758873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-29 06:28:35.009077: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-29 06:28:35.010089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-29 06:28:35.012193: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-10-29 06:28:35.012590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-29 06:28:35.013601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-29 06:28:35.014622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-29 06:28:37.484262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-29 06:28:37.485068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-29 06:28:37.485867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-10-29 06:28:37.486474: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-10-29 06:28:37.486559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10663 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I1029 06:28:37.492064 140433306449792 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: None\n",
            "I1029 06:28:37.498257 140433306449792 config_util.py:552] Maybe overwriting train_steps: None\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I1029 06:28:37.498419 140433306449792 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "I1029 06:28:37.512976 140433306449792 ssd_efficientnet_bifpn_feature_extractor.py:143] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I1029 06:28:37.513164 140433306449792 ssd_efficientnet_bifpn_feature_extractor.py:144] EfficientDet BiFPN num filters: 288\n",
            "I1029 06:28:37.513366 140433306449792 ssd_efficientnet_bifpn_feature_extractor.py:146] EfficientDet BiFPN num iterations: 7\n",
            "I1029 06:28:37.523681 140433306449792 efficientnet_model.py:147] round_filter input=32 output=48\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:37.646878 140433306449792 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:37.648854 140433306449792 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:37.651457 140433306449792 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:37.652637 140433306449792 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:37.660889 140433306449792 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:37.664986 140433306449792 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:37.671269 140433306449792 efficientnet_model.py:147] round_filter input=32 output=48\n",
            "I1029 06:28:37.671404 140433306449792 efficientnet_model.py:147] round_filter input=16 output=24\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:37.686773 140433306449792 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:37.687978 140433306449792 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:37.690146 140433306449792 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:37.691233 140433306449792 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1029 06:28:38.028349 140433306449792 efficientnet_model.py:147] round_filter input=16 output=24\n",
            "I1029 06:28:38.028596 140433306449792 efficientnet_model.py:147] round_filter input=24 output=40\n",
            "I1029 06:28:38.811358 140433306449792 efficientnet_model.py:147] round_filter input=24 output=40\n",
            "I1029 06:28:38.811577 140433306449792 efficientnet_model.py:147] round_filter input=40 output=64\n",
            "I1029 06:28:39.591931 140433306449792 efficientnet_model.py:147] round_filter input=40 output=64\n",
            "I1029 06:28:39.592138 140433306449792 efficientnet_model.py:147] round_filter input=80 output=128\n",
            "I1029 06:28:40.707339 140433306449792 efficientnet_model.py:147] round_filter input=80 output=128\n",
            "I1029 06:28:40.707580 140433306449792 efficientnet_model.py:147] round_filter input=112 output=176\n",
            "I1029 06:28:41.807327 140433306449792 efficientnet_model.py:147] round_filter input=112 output=176\n",
            "I1029 06:28:41.807549 140433306449792 efficientnet_model.py:147] round_filter input=192 output=304\n",
            "I1029 06:28:43.259217 140433306449792 efficientnet_model.py:147] round_filter input=192 output=304\n",
            "I1029 06:28:43.259454 140433306449792 efficientnet_model.py:147] round_filter input=320 output=512\n",
            "I1029 06:28:43.756449 140433306449792 efficientnet_model.py:147] round_filter input=1280 output=2048\n",
            "I1029 06:28:43.962825 140433306449792 efficientnet_model.py:458] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W1029 06:28:44.053874 140433306449792 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:558: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['/content/training_demo/annotations/train.record']\n",
            "I1029 06:28:44.061342 140433306449792 dataset_builder.py:163] Reading unweighted datasets: ['/content/training_demo/annotations/train.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['/content/training_demo/annotations/train.record']\n",
            "I1029 06:28:44.061590 140433306449792 dataset_builder.py:80] Reading record datasets for input file: ['/content/training_demo/annotations/train.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I1029 06:28:44.061794 140433306449792 dataset_builder.py:81] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W1029 06:28:44.061945 140433306449792 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
            "W1029 06:28:44.065490 140433306449792 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W1029 06:28:44.090172 140433306449792 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W1029 06:28:52.600565 140433306449792 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W1029 06:28:57.647203 140433306449792 deprecation.py:345] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2021-10-29 06:29:00.368450: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n",
            "2021-10-29 06:30:27.597554: E tensorflow/stream_executor/cuda/cuda_dnn.cc:362] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
            "2021-10-29 06:30:27.600842: E tensorflow/stream_executor/cuda/cuda_dnn.cc:362] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
            "Traceback (most recent call last):\n",
            "  File \"model_main_tf2.py\", line 115, in <module>\n",
            "    tf.compat.v1.app.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"model_main_tf2.py\", line 112, in main\n",
            "    record_summaries=FLAGS.record_summaries)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 603, in train_loop\n",
            "    train_input, unpad_groundtruth_tensors)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 394, in load_fine_tune_checkpoint\n",
            "    _ensure_model_is_built(model, input_dataset, unpad_groundtruth_tensors)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py\", line 176, in _ensure_model_is_built\n",
            "    labels,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 1286, in run\n",
            "    return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\", line 2849, in call_for_each_replica\n",
            "    return self._call_for_each_replica(fn, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/mirrored_strategy.py\", line 671, in _call_for_each_replica\n",
            "    self._container_strategy(), fn, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/mirrored_run.py\", line 86, in call_for_each_replica\n",
            "    return wrapped(args, kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 885, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 950, in _call\n",
            "    return self._stateless_fn(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 3040, in __call__\n",
            "    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1964, in _call_flat\n",
            "    ctx, args, cancellation_manager=cancellation_manager))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 596, in call\n",
            "    ctx=ctx)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
            "    inputs, attrs, num_outputs)\n",
            "tensorflow.python.framework.errors_impl.UnknownError:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n",
            "\t [[node EfficientDet-D5/model/stem_conv2d/Conv2D (defined at /usr/local/lib/python3.7/dist-packages/object_detection/models/ssd_efficientnet_bifpn_feature_extractor.py:225) ]] [Op:__inference__dummy_computation_fn_49196]\n",
            "\n",
            "Errors may have originated from an input operation.\n",
            "Input Source operations connected to node EfficientDet-D5/model/stem_conv2d/Conv2D:\n",
            " args_1 (defined at /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:176)\n",
            "\n",
            "Function call stack:\n",
            "_dummy_computation_fn\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsxQPmDUuROX",
        "outputId": "ce4f8d14-3892-4f96-f832-b246d6c2a225"
      },
      "source": [
        "!nvcc  --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ue9fpZ4u7yC",
        "outputId": "e917b309-6769-4506-b293-aaa3e3ab7473"
      },
      "source": [
        "!cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n",
        "\n",
        "!cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: /usr/local/cuda/include/cudnn.h: No such file or directory\n",
            "#define CUDNN_MAJOR 7\n",
            "#define CUDNN_MINOR 6\n",
            "#define CUDNN_PATCHLEVEL 5\n",
            "--\n",
            "#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
            "\n",
            "#include \"driver_types.h\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B03zYOwOg40I"
      },
      "source": [
        "Basura:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLrJ-f60cWo1",
        "outputId": "bfff4f55-77db-4172-eedd-345e5689002c"
      },
      "source": [
        "from object_detection.utils import config_util\n",
        "num_classes = 1\n",
        "pipeline_config = \"/content/training_demo/models/primer_modelo/pipeline.config\"\n",
        "\n",
        "\n",
        "# Load pipeline config and build a detection model.\n",
        "#\n",
        "# Since we are working off of a COCO architecture which predicts 90\n",
        "# class slots by default, we override the `num_classes` field here to be just\n",
        "# one (for our new rubber ducky class).\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "model_config.ssd.num_classes = num_classes\n",
        "config_util.save_pipeline_config(model_config, '/content/training_demo/models/primer_modelo')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Writing pipeline config file to /content/training_demo/models/primer_modelo/pipeline.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arBYc5TMdIuq"
      },
      "source": [
        "model_config.ssd.freeze_batchnorm = True\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S3serLtdCwC"
      },
      "source": [
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}